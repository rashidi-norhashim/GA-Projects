{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf4069a",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Using Natural Language Processing (NLP) Modelling to Predict Desktop CPU Brand Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2dc54",
   "metadata": {},
   "source": [
    "# Part 2 - Data Scraping\n",
    "\n",
    "### Contents:\n",
    "- [Data Scraping](#Data-Scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36121cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dbb34",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65fc6ab",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c6bf0",
   "metadata": {},
   "source": [
    "Pushshift API (https://api.pushshift.io/) will be used to obtain 10,000 posts for all the subreddits. These subreddits will then be exported to .csv files for further data cleaning.\n",
    "\n",
    "Pushshift API has some quirks like 1) only allows for 100 posts per API request. 2) has a limit of 60 requests per minute.\n",
    "\n",
    "These quirks were overcome by 1) looping the request (and changing the ```'before'``` parameter with each iteration) in order to get 10,000 posts and 2) adding ```time.sleep(1)``` in order to keep the function within the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d484527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushshift_10k(subreddit_str):\n",
    "    \"\"\"Returns at least 10_000 unique reddit posts with text using PushShift API\"\"\"\n",
    "    #setting the url\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    #Initializing the base parameters\n",
    "    params = {\n",
    "    'subreddit': subreddit_str,\n",
    "    'size': 100,\n",
    "    'fields': ['author','id','subreddit','title','selftext','created_utc'],\n",
    "    }\n",
    "    \n",
    "    #instantiating an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    #utilizing a while loop as long as the number of unique posts with text has not reach 10_000\n",
    "    while df.shape[0] < 10_000:\n",
    "        req = requests.get(url, params)    \n",
    "        if req.status_code == 200:\n",
    "            df_json = req.json()\n",
    "            df_new = pd.DataFrame(df_json['data'])\n",
    "            df_time = df_new.iloc[-1]['created_utc'] #retrieving the last post's timestamp\n",
    "             indexs = df_new[\n",
    "            (df_new['selftext'] == '') |             #removing blank posts\n",
    "            (df_new['selftext'] == '[removed]') |    #removing removed posts\n",
    "            (df_new['selftext'] == '[deleted]') |    #removing deleted posts\n",
    "            (df_new['selftext'].isnull())            #removing NaN posts\n",
    "            ].index\n",
    "            df_new.drop(indexs, axis = 0, inplace = True)\n",
    "            df = pd.concat([df, df_new], axis = 0).reset_index(drop=True)\n",
    "            df.drop_duplicates(subset=['selftext', 'title'], keep='last', inplace=True) #removing duplicated posts\n",
    "            time.sleep(1) #adding a 1 sec pause in between loops\n",
    "        else:\n",
    "            print('Unsuccessful') #returns an error if PushShift API returns an error status code  \n",
    "            return df\n",
    "            break\n",
    "        params['before'] = df_time #adding the time into the 'before' parameter to pull the next 100 posts    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c09ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please uncomment if intend to run\n",
    "# amd_df = pushshift_10k('amd')\n",
    "# amd_df.to_csv('amd_csv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce14c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please uncomment if intend to run\n",
    "#intel_df = pushshift_10k('intel')\n",
    "#intel_df.to_csv('intel_csv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffdc7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please uncomment if intend to run\n",
    "#buildapc_df = pushshift_10k('buildapc')\n",
    "#buildapc_df.to_csv('buildapc_csv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24060a67",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
